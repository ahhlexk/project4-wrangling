{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling\n",
    "## FYI ALL OF THE TEXT BELOW IS DIRECTLY FROM THE JUPYTER NOTEBOOK. IT MIGHT MAKE MORE SENSE READING IT FROM THE JUPYTER NOTEBOOK ALONG WITH THE CODE.\n",
    "\n",
    "In the following cells I perform the necessary steps in order to gather the data from multiple sources. There are three data sources that I will be collecting data from: \n",
    " 1. CSV that was handed to me\n",
    " 2. Downloaded TSV file from online source\n",
    " 3. JSON data from Twitter's API\n",
    "\n",
    "### 1. Downloading and Loading the CSV File into a Dataframe\n",
    "The CSV file was downloaded from Udactiy and stored on my local machine in the same folder location as my Jupyter Notebook. The file was then loaded into a dataframe using pandas as described in the cells below.\n",
    "\n",
    "### 2. Downloading the TSV File from the Internet\n",
    "The next file needed to be downloaded programmatically from the internet using python. A URL was provided where I could download the file. Using the OS and Requests library in python, I was able to create a folder on my machine and make a request to the URL to download the file. \n",
    "\n",
    "After downloading the file, I was able to name the file based on the URL. Then, I stored the data onto a new dataframe using pandas.\n",
    "\n",
    "### 3. Accessing JSON data from Twitter's API\n",
    "The next data set would have to be accessed using Twitter's API. After following the instructions in the class, I was able to setup my own Twitter developer access and get my own authentication and token keys. I followed the instructions on how to access and get data from the Twitter API and used the Tweepy API class to get data. \n",
    "\n",
    "First, I needed to get the Tweet IDs from the Archive file (first file) that I loaded. After getting the Tweet IDs stored in a variable, I ran some simple tests to understand how the data was being returned using a single Tweet ID. I made sure I understood how to use the Tweepy API, get the JSON data, and download the JSON data for one Tweet ID before doing the entire list of tweet IDs. The rest of the steps I took are listed next to the cells below.\n",
    "\n",
    "#### 3a. Downloading all of the JSON data into a text file\n",
    "I stored all of the Tweet IDs from the archive data frame into a list which I would use when sending requests to the Twitter API. I created a simple script to help me loop through all of the Tweet IDs and store each JSON data for each Tweet in its own line in the text file. \n",
    "\n",
    "Because the script took a very long time to run (around 30 minutes), and since the script stores the text file on my local machine after it runs I didn't really need to run it again, I have commented out the script below. \n",
    "\n",
    "#### 3b. Reading the text file line by line and loading the data into a dataframe\n",
    "This next section took the longest time in the wrangling process. Multiple iterations were taken to get the data correctly and I had to look at the JSON data in the text file multiple times for various tweets to ensure that I was getting the data properly.\n",
    "\n",
    "I created the below script to load the text file and then read each line and store the attributes that I wanted into a dataframe. The minimum required attributes were Tweet ID, Retweet Count, and Favorite Count. I had to look through the text file to get the actual attribute names in the JSON data using one Tweet ID as an example. \n",
    "\n",
    "I also decided that I wanted to pull Full Text, URL, and User Mentions just to see if I could. This would prove to be more difficult than I thought. The Full Text was easy to pull as it was just like the other minimum required fields. However, the URL and User Mentions were both nested in the JSON data tree. So, I had to go multiple layers to access both. \n",
    "\n",
    "I noticed that User Mentions were empty for a lot of data (more than 2,000) so I decided to remove that from the script. \n",
    "\n",
    "URL was nested for the majority of tweets in the same location. However, there were some exceptions that came up which is why I added the try-except blocks to the first part of the dataframe script. Some URLs were nested in different parts of the JSON data.\n",
    "\n",
    "#### 3c. Creating a new Dataframe Ignoring Exceptions\n",
    "Next, I wanted to create a new dataframe now that I have all of my exceptions. I wanted to get the list of all the tweet IDs that had the exception then run a modified version of the script I created in step 3b to avoid any exceptions. \n",
    "\n",
    "Getting the URLs for each of the exceptions proved to be challenging as the URL was nested in various places for these tweets. I didn't have enough time to go through each of the exceptions or write the code to handle all scenarios. So, if an exception existed, I just ignored the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
